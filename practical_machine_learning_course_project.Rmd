## Predicting weight lifting quality

<br>

```{r init, echo=F, message=F, warning=F}
### Load packages
pckgs = 
  c('dplyr', 'tidyr', 'ggplot2', 'ggtext', 'hrbrthemes', 'caret', 'randomForest')
invisible(lapply(pckgs, library, character.only = TRUE))

### Load datasets
training = read.csv('pml-training.csv')
testing = read.csv('pml-testing.csv')

training = training %>% 
  select(grep(names(training), 
              pattern = '^accel', #all columns that start with 'accel'
              value = TRUE),
         classe) #get actual column names instead of index

testing = testing %>% 
  select(grep(names(testing), 
              pattern = '^accel', #all columns that start with 'accel'
              value = TRUE), 
         classe) #get actual column names instead of index

### Create actual training data and quiz data
set.seed(123)
inTrain = createDataPartition(y = training$classe, p = 0.7, list = FALSE)
train_set = training[inTrain, ]
quiz_set = training[-inTrain, ]

modrf = readRDS('modrf.rds')
modknn = readRDS('modknn.rds')
modgbm = readRDS('modgbm.rds')
```

### Background of the problem
Nowadays, there's a lot of new technologies that can easily track the statistics of your daily routine from standing, sitting, walking, running, and so on. The data that were generated by these tools can be used to predict activity type that the person is doing. See this example [paper](https://www.researchgate.net/publication/233948453_Detection_of_Physical_Activity_Types_Using_Triaxial_Accelerometers) that attempts to detect activity type using triaxial accelerometers.

In this study, we will be working on **weight lifting exercises dataset**. The study was conducted by **Velloso**, **Bulling**, **Gellersen**, **Ugulino**, and **Fuks**. You can read more about their paper [here](http://web.archive.org/web/20170519033209/http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf). Six participants had performed 10 repetitions of **unilateral dumbbell biceps curl**. They performed the activity in five different ways. One of which is the proper way of executing the exercise (denoted by **class A**), while the rest corresponds to the common mistakes when doing the activity (other classes **B-E**). Participants had IMUs on them at specific points to measure triaxial accelerometers for each activity classification.

This study focuses on predicting **the manner** how this exercise is conducted using the data collected from the experiment. Specifically, we will use the data from **accelerometers** on the **belt**, **forearm**, **arm**, and **dumbbell**.

<br>

### Data preparation and model selection
The training data consists of **19,622** rows and **160** columns. As mentioned, the features that will only be used are the **accelerometers** on the **belt**, **forearm**, **arm**, and **dumbbell**. Selecting only these columns will give us **12 features** (13 columns in total including the `classe` column, our outcome variable). In the testing set, there's only **20** rows, which is really intended for the final quiz in <b>[Practical Machine Learning](https://www.coursera.org/learn/practical-machine-learning)</b> course by **John Hopkins University** in <b>[Coursera](https://www.coursera.org/)</b>, but we will use it as our actual test set.

Training set was partitioned into two - one for the **actual training** set and one for the **quiz set**. **70% (13,737 rows)** of the full training set was used to train the models, while the remaining **30% (5,885 rows)** was used to test their accuracy. Our quiz set is our *pseudo test set* in this study. For the sake of reproducibility, `set.seed` function was used.

The approach that I did is to first train the models **without data pre-processing**. I set a threshold of **90% accuracy rate** on the quiz set for a model to be used in the test set. If none met the requirement, then, **pre-processing** will be conducted. <b>[Random forest](https://en.wikipedia.org/wiki/Random_forest#:~:text=Random%20forests%20or%20random%20decision,average%20prediction%20(regression)%20of%20the)</b>, <b>[K-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#:~:text=In%20k%2DNN%20classification%2C%20the,positive%20integer%2C%20typically%20small) (k-NN)</b>, and <b>[gradient boosting machine (GBM)](https://en.wikipedia.org/wiki/Gradient_boosting)</b> are the models used in this study. These models used `repeatedcv` as resampling method with 3 folds and 3 repeats. 

<br>

### Training the models and testing accuracies
In a separate script, the models were fit, and the resulting objects were saved. Below graph illustrates the accuracy rate per model on both training and test sets.
```{r accu, message=F, echo=F, fig.width=10, fig.height=7, warning=F}
rf_train_accu = mean(predict(modrf$finalModel) == factor(train_set$classe))
rf_quiz_accu = mean(predict(modrf$finalModel, quiz_set) == factor(quiz_set$classe))

knn_train_accu = mean(predict(modknn) == factor(train_set$classe))
knn_quiz_accu = mean(predict(modknn, quiz_set) == factor(quiz_set$classe))

gbm_train_accu = mean(predict(modgbm) == factor(train_set$classe))
gbm_quiz_accu = mean(predict(modgbm, quiz_set) == factor(quiz_set$classe))

mod_labs = c('Random Forest', 'k-NN', 'GBM')
accu_labs = c('train', 'quiz')
man_col = c('Training' = 'green', 'Quiz' = 'darkred')

accu_df = data.frame(expand.grid(model = mod_labs, type = accu_labs), 
                     accuracy = c(rf_train_accu, knn_train_accu, gbm_train_accu, 
                                  rf_quiz_accu, knn_quiz_accu, gbm_quiz_accu)) 

accu_df %>% 
        spread(type, accuracy) %>% 
        group_by(model) %>%
        ggplot() +
        geom_segment(aes(x = train, xend = quiz, y = model, yend = model), 
                     color = "gray80", size = 4) + 
        geom_point(aes(x = train, y = model, color = 'Training'), size = 7) + 
        geom_point(aes(x = quiz, y = model, color = 'Quiz'), size = 7) + 
        theme_ipsum(grid = 'X') + 
        geom_text(aes(x = train, y = model, label = paste(round(train, 4) * 100, '%', sep ='')), nudge_x = 0.01) + 
        geom_text(aes(x = quiz, y = model, label = paste(round(quiz, 4) * 100, '%', sep = '')), nudge_x = -0.01) + 
        scale_x_comma(position = "top", limits = c(0.78, 0.96)) +
        labs(x = 'Prediction Accuracy', color = 'Legend', 
             title = 'Prediction accuracy on training and quiz sets', 
             subtitle = 'Significant overfitting observed on k-NN and GBM. <b>Random Forest</b> accuracy on Training and Quiz sets are <b>relatively the same</b>.') + 
        scale_color_manual(values = man_col) + 
        theme(legend.position = 'top', axis.text.x = element_blank(), text = element_text(),
              plot.title.position = "plot",
              plot.title = element_text(size = 15, face= "bold"),
              plot.subtitle = element_markdown(size = 12))

```

From the above visual, we can see that using **GBM** and **k-NN** led to overfitting with a variance of **3** and **8%** respectively. Using **random forest**, however, resulted to accuracy rates that are nearly equal and has only **0.02%** gap between training and quiz accuracy rate. Following the threshold that we previously set, we will use random forest to predict the test set.

<br> 

### Variable Importance
Variable importance is basically the measure of *usefulness* of a variable on a model. We can get the variable importance for each model that we used. I'll just present the variable importance of our selected model, that is, random forest.
```{r varImp, echo=F, message=F, fig.width=10, fig.height=7, warning=F}
rf_varImp = data.table::setDT(
  varImp(modrf)$importance, keep.rownames = T) %>% 
  mutate(ranking = rank(-Overall)) %>% 
  rename(variables = rn) %>% 
  arrange(ranking)

v1 = rf_varImp %>% 
  ggplot(aes(x = reorder(variables, Overall), y = Overall)) +
  geom_bar(stat = 'identity', fill = 'steelblue') + 
  coord_flip() +
  theme_ipsum(grid = 'X') + 
  labs(x = 'Variables', y = 'Importance', title = 'Variable importance on Random Forest', 
       subtitle = 'Accelerometers on <b>belt (z)</b> and <b>dumbbell (y and z)</b> are the top <b>most useful variables</b> in the selected model.') + 
  geom_text(aes(x = variables, y = Overall, label = round(Overall, 2)), nudge_y = 2) + 
  scale_y_comma(position = "right", limits = c(0, 103)) +
  theme(legend.position = 'top', axis.text.x = element_blank(), text = element_text(),
              plot.title.position = "plot",
              plot.title = element_text(size = 15, face= "bold"),
              plot.subtitle = element_markdown(size = 12))

v1
```

It's obvious that the accelerometers on **belt** on z angle, and **dumbbell** on y and z angles are the top 3 most useful variables in the data. Variable importance can be considered in feature selection to improve the model. Here, we can set a threshold or select the n-top most important features from the data to build another model aiming for better accuracy.

<br>

### Prediction on test data
```{r test_pred, echo=F, message=F, warning=F, fig.width=10, fig.height=7}
testing$pred = predict(modrf, testing)
testing$bool = ifelse(testing$classe == testing$pred, 'correct', 'incorrect')
man_col = c('correct' = 'steelblue', 'incorrect' = 'darkred')

testing %>% 
  ggplot(aes(x = accel_belt_z, y = accel_dumbbell_y, color = bool)) + 
  geom_point(size = 4, alpha = 0.7) + 
  scale_color_manual(values = man_col) + 
  theme_ipsum(grid = 'X') + 
  theme(legend.position = 'top', plot.title.position = "plot",
              plot.title = element_text(size = 15, face= "bold"),
              plot.subtitle = element_markdown(size = 12)) + 
  scale_x_comma(position = "top", limits = c(-200, 50)) + 
  ylim(c(-50, 175)) + 
  labs(color = 'Prediction', title = 'Predicting the manner of how unilateral dumbbell biceps curl was done', subtitle = 'Using <b>Random Forest</b> yielded <b>95% accuracy rate</b> on the testing data') + 
  annotate(geom = 'curve', x = 25, xend = 49, y = 125, yend = 152, curvature = 1,
           arrow = arrow(length = unit(2, 'mm')), size = 1, color = 'darkred') + 
  annotate(geom = 'text', x = 5, y = 125, label = 'Incorrect prediction', color = 'darkred', fontface = 'bold')
```

The plot above is the scatterplot of `accel_dumbbell_y` and `accel_belt_z` which was colored by correct and incorrect predictions. Predicting the `classe` variable from the testing set using **random forest** gave us **95% accuracy rate** or **19 correct predictions** out of 20 observations. From the chart above, it's not too obvious but the incorrect prediction was masked by a correct prediction. Looking closely, the dot which was pointed out by the arrow is a little darker because of the mixed blue and red plot with some transparency. And looking at the actual data below tells us that the there are two points with the same exact values of `accel_belt_z` and `accel_dumbbell_y`.
```{r tab, message=F, warning=F}
testing %>% filter(accel_belt_z == 49) %>% select(bool, accel_belt_z, accel_dumbbell_y)
```

<br>

### Conclusion and recommendations

Having equal values on the two most important variables might have been the reason of this incorrect prediction. Setting aside data error, we could've worked it out by either fitting the same model using the most important features, or we could use other models but different tuning parameters and applying some feature selection. We can also use combined models or **ensemble**. The other two models we've fit had fairly high prediction rates on quiz set. Combining these three models could yield an even higher prediction. 

Overall, the final model that we've used performed really well. 






